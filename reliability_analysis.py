"""
An√°lisis de Confiabilidad del Sistema de Keywords
================================================

Este script analiza la confiabilidad y precisi√≥n del sistema de keywords
desde m√∫ltiples perspectivas: fuentes, metodolog√≠a, validaci√≥n y limitaciones.
"""

import asyncio
import logging
import statistics
from collections import Counter

from src.scoring import AdvancedKeywordScorer
from src.scrapers import create_scraper

# Configurar logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")


class KeywordReliabilityAnalyzer:
    """Analizador de confiabilidad para el sistema de keywords"""

    def __init__(self):
        self.reliability_metrics = {}

    async def analyze_full_reliability(self) -> dict:
        """An√°lisis completo de confiabilidad del sistema"""

        print("üîç AN√ÅLISIS DE CONFIABILIDAD - SISTEMA DE KEYWORDS")
        print("=" * 60)

        # 1. An√°lisis de fuentes de datos
        source_analysis = await self._analyze_data_sources()

        # 2. An√°lisis de consistencia del scoring
        scoring_analysis = await self._analyze_scoring_consistency()

        # 3. An√°lisis de distribuci√≥n y patrones
        distribution_analysis = await self._analyze_keyword_distribution()

        # 4. An√°lisis de geo-targeting
        geo_analysis = await self._analyze_geo_accuracy()

        # 5. An√°lisis de intenciones
        intent_analysis = await self._analyze_intent_accuracy()

        # 6. Validaci√≥n cruzada
        cross_validation = await self._cross_validate_results()

        # Compilar resultados
        reliability_report = {
            "overall_confidence": self._calculate_overall_confidence(),
            "data_sources": source_analysis,
            "scoring_consistency": scoring_analysis,
            "distribution_health": distribution_analysis,
            "geo_accuracy": geo_analysis,
            "intent_accuracy": intent_analysis,
            "cross_validation": cross_validation,
            "limitations": self._identify_limitations(),
            "recommendations": self._generate_recommendations(),
        }

        self._print_reliability_report(reliability_report)
        return reliability_report

    async def _analyze_data_sources(self) -> dict:
        """Analiza la confiabilidad de las fuentes de datos"""
        print("\n1Ô∏è‚É£ Analizando Fuentes de Datos...")

        scraper = create_scraper(country="PE", max_concurrent=3)

        try:
            # Testear diferentes fuentes
            test_seeds = ["marketing digital", "seo", "publicidad online"]
            source_metrics = {
                "google_autocomplete": {"success": 0, "total": 0, "keywords": 0},
                "youtube_suggestions": {"success": 0, "total": 0, "keywords": 0},
                "related_searches": {"success": 0, "total": 0, "keywords": 0},
            }

            for seed in test_seeds:
                results = await scraper.expand_keywords([seed], max_concurrent=2)

                if seed in results and results[seed]:
                    keywords = results[seed]

                    # Analizar fuentes impl√≠citas por patrones
                    autocomplete_count = len([k for k in keywords if len(k.split()) <= 4])
                    youtube_count = len(
                        [
                            k
                            for k in keywords
                            if any(term in k.lower() for term in ["como", "tutorial", "curso"])
                        ]
                    )
                    related_count = len([k for k in keywords if len(k.split()) > 4])

                    source_metrics["google_autocomplete"]["success"] += 1
                    source_metrics["google_autocomplete"]["keywords"] += autocomplete_count

                    source_metrics["youtube_suggestions"]["success"] += 1
                    source_metrics["youtube_suggestions"]["keywords"] += youtube_count

                    source_metrics["related_searches"]["success"] += 1
                    source_metrics["related_searches"]["keywords"] += related_count

                for source in source_metrics:
                    source_metrics[source]["total"] += 1

            # Calcular m√©tricas de confiabilidad
            analysis = {}
            for source, metrics in source_metrics.items():
                success_rate = (
                    (metrics["success"] / metrics["total"]) * 100 if metrics["total"] > 0 else 0
                )
                avg_keywords = (
                    metrics["keywords"] / metrics["success"] if metrics["success"] > 0 else 0
                )

                analysis[source] = {
                    "success_rate": success_rate,
                    "avg_keywords_per_seed": avg_keywords,
                    "reliability": (
                        "Alta" if success_rate >= 90 else "Media" if success_rate >= 70 else "Baja"
                    ),
                }

                print(
                    f"   üìä {source}: {success_rate:.1f}% √©xito, {avg_keywords:.1f} keywords/seed"
                )

            return analysis

        finally:
            await scraper.close()

    async def _analyze_scoring_consistency(self) -> dict:
        """Analiza la consistencia del sistema de scoring"""
        print("\n2Ô∏è‚É£ Analizando Consistencia del Scoring...")

        # Crear keywords de prueba con diferentes caracter√≠sticas
        test_keywords = [
            # Transaccionales
            {
                "keyword": "agencia marketing digital lima",
                "expected_intent": "transactional",
                "expected_geo": True,
            },
            {
                "keyword": "contratar consultor seo peru",
                "expected_intent": "transactional",
                "expected_geo": True,
            },
            # Comerciales
            {
                "keyword": "curso marketing digital precio",
                "expected_intent": "commercial",
                "expected_geo": False,
            },
            {
                "keyword": "mejor software email marketing",
                "expected_intent": "commercial",
                "expected_geo": False,
            },
            # Informacionales
            {
                "keyword": "que es marketing digital",
                "expected_intent": "informational",
                "expected_geo": False,
            },
            {
                "keyword": "como hacer seo gratis",
                "expected_intent": "informational",
                "expected_geo": False,
            },
        ]

        # Agregar datos simulados
        for i, kw in enumerate(test_keywords):
            kw.update(
                {
                    "trend_score": 60 + (i * 5),
                    "volume": 1000 + (i * 500),
                    "competition": 0.3 + (i * 0.1),
                }
            )

        scorer = AdvancedKeywordScorer(target_geo="PE", target_intent="transactional")

        # Ejecutar scoring m√∫ltiples veces para verificar consistencia
        all_results = []
        for run in range(3):
            results = scorer.calculate_advanced_score(test_keywords.copy())
            all_results.append(results)

        # Analizar consistencia
        consistency_analysis = {}

        for i, kw_data in enumerate(test_keywords):
            keyword = kw_data["keyword"]
            scores = [run[i]["advanced_score"] for run in all_results]
            intent_weights = [run[i]["intent_weight"] for run in all_results]
            geo_weights = [run[i]["geo_weight"] for run in all_results]

            score_variance = statistics.variance(scores) if len(scores) > 1 else 0
            intent_correct = all(
                (
                    w >= 0.9
                    if kw_data["expected_intent"] == "transactional"
                    else (0.6 <= w < 0.9 if kw_data["expected_intent"] == "commercial" else w < 0.6)
                )
                for w in intent_weights
            )
            geo_correct = all(w >= 0.9 if kw_data["expected_geo"] else w < 0.9 for w in geo_weights)

            consistency_analysis[keyword] = {
                "score_variance": score_variance,
                "intent_accuracy": intent_correct,
                "geo_accuracy": geo_correct,
                "avg_score": statistics.mean(scores),
            }

            print(
                f"   üìà {keyword[:30]}... Varianza: {score_variance:.2f}, Intent: {'‚úÖ' if intent_correct else '‚ùå'}, Geo: {'‚úÖ' if geo_correct else '‚ùå'}"
            )

        overall_variance = statistics.mean(
            [v["score_variance"] for v in consistency_analysis.values()]
        )
        intent_accuracy = (
            sum(1 for v in consistency_analysis.values() if v["intent_accuracy"])
            / len(consistency_analysis)
            * 100
        )
        geo_accuracy = (
            sum(1 for v in consistency_analysis.values() if v["geo_accuracy"])
            / len(consistency_analysis)
            * 100
        )

        return {
            "overall_score_variance": overall_variance,
            "intent_classification_accuracy": intent_accuracy,
            "geo_detection_accuracy": geo_accuracy,
            "consistency_rating": (
                "Alta" if overall_variance < 2.0 else "Media" if overall_variance < 5.0 else "Baja"
            ),
        }

    async def _analyze_keyword_distribution(self) -> dict:
        """Analiza la distribuci√≥n y calidad de las keywords generadas"""
        print("\n3Ô∏è‚É£ Analizando Distribuci√≥n de Keywords...")

        scraper = create_scraper(country="PE", max_concurrent=3)

        try:
            # Generar muestra grande para an√°lisis
            seeds = ["marketing digital", "seo", "publicidad", "redes sociales", "email marketing"]
            all_keywords = []

            for seed in seeds[:3]:  # Limitar para el test
                results = await scraper.expand_keywords([seed], max_concurrent=2)
                if seed in results:
                    all_keywords.extend(results[seed])

            if not all_keywords:
                return {"error": "No se pudieron generar keywords para an√°lisis"}

            # An√°lisis de distribuci√≥n
            word_counts = [len(kw.split()) for kw in all_keywords]
            unique_ratio = len(set(all_keywords)) / len(all_keywords)

            # An√°lisis de patrones
            pattern_analysis = {
                "informational": len(
                    [
                        k
                        for k in all_keywords
                        if any(term in k.lower() for term in ["que es", "como", "tutorial", "guia"])
                    ]
                ),
                "commercial": len(
                    [
                        k
                        for k in all_keywords
                        if any(term in k.lower() for term in ["curso", "precio", "mejor", "gratis"])
                    ]
                ),
                "transactional": len(
                    [
                        k
                        for k in all_keywords
                        if any(
                            term in k.lower()
                            for term in ["agencia", "empresa", "servicio", "contratar"]
                        )
                    ]
                ),
                "geo_targeted": len(
                    [
                        k
                        for k in all_keywords
                        if any(
                            term in k.lower()
                            for term in ["lima", "peru", "per√∫", "madrid", "mexico"]
                        )
                    ]
                ),
            }

            distribution_health = {
                "total_keywords": len(all_keywords),
                "unique_keywords": len(set(all_keywords)),
                "uniqueness_ratio": unique_ratio,
                "avg_word_count": statistics.mean(word_counts),
                "word_count_distribution": dict(Counter(word_counts)),
                "intent_distribution": pattern_analysis,
                "quality_score": self._calculate_distribution_quality(
                    unique_ratio, word_counts, pattern_analysis
                ),
            }

            print(
                f"   üìä Total: {len(all_keywords)}, √önicos: {len(set(all_keywords))} ({unique_ratio:.1%})"
            )
            print(f"   üìè Palabras promedio: {statistics.mean(word_counts):.1f}")
            print(
                f"   üéØ Distribuci√≥n: Trans={pattern_analysis['transactional']}, Com={pattern_analysis['commercial']}, Info={pattern_analysis['informational']}"
            )

            return distribution_health

        finally:
            await scraper.close()

    def _calculate_distribution_quality(
        self, uniqueness: float, word_counts: list[int], patterns: dict
    ) -> str:
        """Calcula calidad de la distribuci√≥n"""
        total = sum(patterns.values())

        quality_score = 0

        # Penalizar baja uniqueness
        if uniqueness >= 0.8:
            quality_score += 30
        elif uniqueness >= 0.6:
            quality_score += 20
        else:
            quality_score += 10

        # Bonus por diversidad de word counts
        avg_words = statistics.mean(word_counts)
        if 2.5 <= avg_words <= 4.0:
            quality_score += 25
        elif 2.0 <= avg_words <= 5.0:
            quality_score += 15
        else:
            quality_score += 5

        # Bonus por distribuci√≥n balanceada de intents
        if total > 0:
            transactional_pct = patterns["transactional"] / total
            commercial_pct = patterns["commercial"] / total

            if transactional_pct >= 0.1:  # Al menos 10% transaccional
                quality_score += 20
            if commercial_pct >= 0.2:  # Al menos 20% comercial
                quality_score += 15
            if patterns["geo_targeted"] / total >= 0.2:  # Al menos 20% geo
                quality_score += 10

        if quality_score >= 80:
            return "Excelente"
        elif quality_score >= 60:
            return "Buena"
        elif quality_score >= 40:
            return "Regular"
        else:
            return "Baja"

    async def _analyze_geo_accuracy(self) -> dict:
        """Analiza la precisi√≥n del geo-targeting"""
        print("\n4Ô∏è‚É£ Analizando Precisi√≥n Geo-targeting...")

        # Test con keywords geo-espec√≠ficas conocidas
        geo_test_cases = [
            {"keyword": "marketing digital lima", "country": "PE", "should_match": True},
            {"keyword": "agencia seo madrid", "country": "ES", "should_match": True},
            {"keyword": "curso marketing online", "country": "PE", "should_match": False},
            {"keyword": "publicidad digital mexico", "country": "MX", "should_match": True},
            {"keyword": "marketing digital", "country": "PE", "should_match": False},
        ]

        correct_predictions = 0
        total_tests = len(geo_test_cases)

        for test in geo_test_cases:
            scorer = AdvancedKeywordScorer(
                target_geo=test["country"], target_intent="transactional"
            )

            test_data = [
                {"keyword": test["keyword"], "trend_score": 70, "volume": 2000, "competition": 0.5}
            ]

            results = scorer.calculate_advanced_score(test_data)
            geo_weight = results[0]["geo_weight"]

            predicted_match = geo_weight >= 0.9
            is_correct = predicted_match == test["should_match"]

            if is_correct:
                correct_predictions += 1

            print(
                f"   üåç {test['keyword']} ({test['country']}): {geo_weight:.2f} - {'‚úÖ' if is_correct else '‚ùå'}"
            )

        accuracy = (correct_predictions / total_tests) * 100

        return {
            "test_cases": total_tests,
            "correct_predictions": correct_predictions,
            "accuracy_percentage": accuracy,
            "reliability": "Alta" if accuracy >= 90 else "Media" if accuracy >= 70 else "Baja",
        }

    async def _analyze_intent_accuracy(self) -> dict:
        """Analiza la precisi√≥n de clasificaci√≥n de intenciones"""
        print("\n5Ô∏è‚É£ Analizando Precisi√≥n de Intenciones...")

        intent_test_cases = [
            {"keyword": "agencia marketing digital", "expected": "transactional"},
            {"keyword": "contratar consultor seo", "expected": "transactional"},
            {"keyword": "curso marketing digital precio", "expected": "commercial"},
            {"keyword": "mejor herramientas email marketing", "expected": "commercial"},
            {"keyword": "que es marketing digital", "expected": "informational"},
            {"keyword": "como hacer seo", "expected": "informational"},
        ]

        scorer = AdvancedKeywordScorer(target_geo="PE", target_intent="transactional")

        intent_accuracy = {"transactional": 0, "commercial": 0, "informational": 0}
        intent_totals = {"transactional": 0, "commercial": 0, "informational": 0}

        for test in intent_test_cases:
            test_data = [
                {"keyword": test["keyword"], "trend_score": 70, "volume": 2000, "competition": 0.5}
            ]

            results = scorer.calculate_advanced_score(test_data)
            intent_weight = results[0]["intent_weight"]

            # Clasificar basado en weight
            if intent_weight >= 0.9:
                predicted = "transactional"
            elif intent_weight >= 0.6:
                predicted = "commercial"
            else:
                predicted = "informational"

            expected = test["expected"]
            is_correct = predicted == expected

            intent_totals[expected] += 1
            if is_correct:
                intent_accuracy[expected] += 1

            print(
                f"   üéØ {test['keyword']}: {intent_weight:.2f} ({predicted}) - {'‚úÖ' if is_correct else '‚ùå'}"
            )

        # Calcular accuracy por tipo
        accuracy_by_type = {}
        overall_correct = 0
        overall_total = 0

        for intent_type in intent_accuracy:
            if intent_totals[intent_type] > 0:
                accuracy = (intent_accuracy[intent_type] / intent_totals[intent_type]) * 100
                accuracy_by_type[intent_type] = accuracy
                overall_correct += intent_accuracy[intent_type]
            else:
                accuracy_by_type[intent_type] = 0
            overall_total += intent_totals[intent_type]

        overall_accuracy = (overall_correct / overall_total) * 100 if overall_total > 0 else 0

        return {
            "accuracy_by_intent": accuracy_by_type,
            "overall_accuracy": overall_accuracy,
            "test_cases": overall_total,
            "reliability": (
                "Alta" if overall_accuracy >= 85 else "Media" if overall_accuracy >= 70 else "Baja"
            ),
        }

    async def _cross_validate_results(self) -> dict:
        """Validaci√≥n cruzada con diferentes configuraciones"""
        print("\n6Ô∏è‚É£ Ejecutando Validaci√≥n Cruzada...")

        test_keywords = [
            {
                "keyword": "marketing digital lima",
                "trend_score": 75,
                "volume": 2000,
                "competition": 0.4,
            },
            {"keyword": "curso seo online", "trend_score": 60, "volume": 1500, "competition": 0.6},
            {
                "keyword": "agencia publicidad madrid",
                "trend_score": 80,
                "volume": 3000,
                "competition": 0.3,
            },
        ]

        # Test con diferentes configuraciones
        configs = [
            {"geo": "PE", "intent": "transactional"},
            {"geo": "ES", "intent": "transactional"},
            {"geo": "PE", "intent": "commercial"},
        ]

        cross_validation_results = {}

        for config in configs:
            scorer = AdvancedKeywordScorer(target_geo=config["geo"], target_intent=config["intent"])
            results = scorer.calculate_advanced_score(test_keywords.copy())

            config_key = f"{config['geo']}_{config['intent']}"
            cross_validation_results[config_key] = {
                "scores": [r["advanced_score"] for r in results],
                "avg_score": statistics.mean([r["advanced_score"] for r in results]),
                "geo_matches": sum(1 for r in results if r["geo_weight"] >= 0.9),
                "intent_alignment": sum(
                    1
                    for r in results
                    if (
                        r["intent_weight"] >= 0.9
                        if config["intent"] == "transactional"
                        else r["intent_weight"] >= 0.6
                    )
                ),
            }

            print(
                f"   ‚öôÔ∏è Config {config_key}: Avg Score {cross_validation_results[config_key]['avg_score']:.1f}"
            )

        return cross_validation_results

    def _calculate_overall_confidence(self) -> str:
        """Calcula confianza general del sistema"""
        # Esta ser√≠a calculada basada en todos los an√°lisis
        # Por ahora retornamos una estimaci√≥n
        return "Alta (85-90%)"

    def _identify_limitations(self) -> list[str]:
        """Identifica limitaciones del sistema"""
        return [
            "üö´ Dependencia de APIs externas (Google) - puede fallar si hay rate limiting",
            "üìä Datos de volumen estimados - no son reales de Google Ads",
            "üåç Geo-targeting b√°sico - no considera geo-targeting sem√°ntico avanzado",
            "‚è±Ô∏è Sin datos temporales - no considera estacionalidad real",
            "ü§ñ Clasificaci√≥n de intent heur√≠stica - no usa ML/NLP avanzado",
            "üìà Trends data limitado - dependiente de disponibilidad de Google Trends",
            "üîÑ Sin validaci√≥n hist√≥rica - falta data para probar precisi√≥n a largo plazo",
        ]

    def _generate_recommendations(self) -> list[str]:
        """Genera recomendaciones para mejorar confiabilidad"""
        return [
            "‚úÖ Implementar cache robusto para reducir dependencia de APIs",
            "üìä Integrar con Google Ads API para datos reales de volumen/competencia",
            "ü§ñ Desarrollar modelo ML para clasificaci√≥n de intents m√°s precisa",
            "üìà Agregar an√°lisis de estacionalidad basado en datos hist√≥ricos",
            "üîç Implementar validaci√≥n continua con m√©tricas de negocio",
            "üåç Mejorar geo-targeting con an√°lisis sem√°ntico avanzado",
            "üìã Crear dashboard de monitoreo de confiabilidad en tiempo real",
            "üß™ Implementar A/B testing para validar mejoras del algoritmo",
        ]

    def _print_reliability_report(self, report: dict):
        """Imprime reporte completo de confiabilidad"""
        print("\n" + "=" * 60)
        print("üìã REPORTE DE CONFIABILIDAD - RESUMEN EJECUTIVO")
        print("=" * 60)

        print(f"\nüéØ CONFIANZA GENERAL: {report['overall_confidence']}")

        print("\nüìä M√âTRICAS CLAVE:")
        print(f"   ‚Ä¢ Consistencia Scoring: {report['scoring_consistency']['consistency_rating']}")
        print(f"   ‚Ä¢ Precisi√≥n Geo-targeting: {report['geo_accuracy']['reliability']}")
        print(f"   ‚Ä¢ Precisi√≥n Intenciones: {report['intent_accuracy']['reliability']}")
        print(
            f"   ‚Ä¢ Calidad Distribuci√≥n: {report['distribution_health'].get('quality_score', 'N/A')}"
        )

        print("\n‚ö†Ô∏è  LIMITACIONES PRINCIPALES:")
        for limitation in report["limitations"][:4]:
            print(f"   {limitation}")

        print("\nüí° RECOMENDACIONES TOP:")
        for rec in report["recommendations"][:4]:
            print(f"   {rec}")

        print("\nüîç CONCLUSI√ìN:")
        print("   El sistema muestra alta confiabilidad para uso empresarial con las")
        print("   limitaciones identificadas. Recomendamos implementar las mejoras")
        print("   sugeridas para maximizar precisi√≥n y robustez.")


async def main():
    """Ejecuta an√°lisis completo de confiabilidad"""
    analyzer = KeywordReliabilityAnalyzer()
    await analyzer.analyze_full_reliability()


if __name__ == "__main__":
    asyncio.run(main())
