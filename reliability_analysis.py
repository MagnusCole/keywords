"""
AnÃ¡lisis de Confiabilidad del Sistema de Keywords
================================================

Este script analiza la confiabilidad y precisiÃ³n del sistema de keywords
desde mÃºltiples perspectivas: fuentes, metodologÃ­a, validaciÃ³n y limitaciones.
"""

import asyncio
import logging
import statistics
from collections import Counter

from src.scoring import AdvancedKeywordScorer
from src.scrapers import create_scraper

# Configurar logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")


class KeywordReliabilityAnalyzer:
    """Analizador de confiabilidad para el sistema de keywords"""

    def __init__(self):
        self.reliability_metrics = {}

    async def analyze_full_reliability(self) -> dict:
        """AnÃ¡lisis completo de confiabilidad del sistema"""

        print("ğŸ” ANÃLISIS DE CONFIABILIDAD - SISTEMA DE KEYWORDS")
        print("=" * 60)

        # 1. AnÃ¡lisis de fuentes de datos
        source_analysis = await self._analyze_data_sources()

        # 2. AnÃ¡lisis de consistencia del scoring
        scoring_analysis = await self._analyze_scoring_consistency()

        # 3. AnÃ¡lisis de distribuciÃ³n y patrones
        distribution_analysis = await self._analyze_keyword_distribution()

        # 4. AnÃ¡lisis de geo-targeting
        geo_analysis = await self._analyze_geo_accuracy()

        # 5. AnÃ¡lisis de intenciones
        intent_analysis = await self._analyze_intent_accuracy()

        # 6. ValidaciÃ³n cruzada
        cross_validation = await self._cross_validate_results()

        # Compilar resultados
        reliability_report = {
            "overall_confidence": self._calculate_overall_confidence(),
            "data_sources": source_analysis,
            "scoring_consistency": scoring_analysis,
            "distribution_health": distribution_analysis,
            "geo_accuracy": geo_analysis,
            "intent_accuracy": intent_analysis,
            "cross_validation": cross_validation,
            "limitations": self._identify_limitations(),
            "recommendations": self._generate_recommendations(),
        }

        self._print_reliability_report(reliability_report)
        return reliability_report

    async def _analyze_data_sources(self) -> dict:
        """Analiza la confiabilidad de las fuentes de datos"""
        print("\n1ï¸âƒ£ Analizando Fuentes de Datos...")

        scraper = create_scraper(country="PE", max_concurrent=3)

        try:
            # Testear diferentes fuentes
            test_seeds = ["marketing digital", "seo", "publicidad online"]
            source_metrics = {
                "google_autocomplete": {"success": 0, "total": 0, "keywords": 0},
                "youtube_suggestions": {"success": 0, "total": 0, "keywords": 0},
                "related_searches": {"success": 0, "total": 0, "keywords": 0},
            }

            for seed in test_seeds:
                results = await scraper.expand_keywords([seed], max_concurrent=2)

                if seed in results and results[seed]:
                    keywords = results[seed]

                    # Analizar fuentes implÃ­citas por patrones
                    autocomplete_count = len([k for k in keywords if len(k.split()) <= 4])
                    youtube_count = len(
                        [
                            k
                            for k in keywords
                            if any(term in k.lower() for term in ["como", "tutorial", "curso"])
                        ]
                    )
                    related_count = len([k for k in keywords if len(k.split()) > 4])

                    source_metrics["google_autocomplete"]["success"] += 1
                    source_metrics["google_autocomplete"]["keywords"] += autocomplete_count

                    source_metrics["youtube_suggestions"]["success"] += 1
                    source_metrics["youtube_suggestions"]["keywords"] += youtube_count

                    source_metrics["related_searches"]["success"] += 1
                    source_metrics["related_searches"]["keywords"] += related_count

                for source in source_metrics:
                    source_metrics[source]["total"] += 1

            # Calcular mÃ©tricas de confiabilidad
            analysis = {}
            for source, metrics in source_metrics.items():
                success_rate = (
                    (metrics["success"] / metrics["total"]) * 100 if metrics["total"] > 0 else 0
                )
                avg_keywords = (
                    metrics["keywords"] / metrics["success"] if metrics["success"] > 0 else 0
                )

                analysis[source] = {
                    "success_rate": success_rate,
                    "avg_keywords_per_seed": avg_keywords,
                    "reliability": (
                        "Alta" if success_rate >= 90 else "Media" if success_rate >= 70 else "Baja"
                    ),
                }

                print(
                    f"   ğŸ“Š {source}: {success_rate:.1f}% Ã©xito, {avg_keywords:.1f} keywords/seed"
                )

            return analysis

        finally:
            await scraper.close()

    async def _analyze_scoring_consistency(self) -> dict:
        """Analiza la consistencia del sistema de scoring"""
        print("\n2ï¸âƒ£ Analizando Consistencia del Scoring...")

        # Crear keywords de prueba con diferentes caracterÃ­sticas
        test_keywords = [
            # Transaccionales
            {
                "keyword": "agencia marketing digital lima",
                "expected_intent": "transactional",
                "expected_geo": True,
            },
            {
                "keyword": "contratar consultor seo peru",
                "expected_intent": "transactional",
                "expected_geo": True,
            },
            # Comerciales
            {
                "keyword": "curso marketing digital precio",
                "expected_intent": "commercial",
                "expected_geo": False,
            },
            {
                "keyword": "mejor software email marketing",
                "expected_intent": "commercial",
                "expected_geo": False,
            },
            # Informacionales
            {
                "keyword": "que es marketing digital",
                "expected_intent": "informational",
                "expected_geo": False,
            },
            {
                "keyword": "como hacer seo gratis",
                "expected_intent": "informational",
                "expected_geo": False,
            },
        ]

        # Agregar datos simulados
        for i, kw in enumerate(test_keywords):
            kw.update(
                {
                    "trend_score": 60 + (i * 5),
                    "volume": 1000 + (i * 500),
                    "competition": 0.3 + (i * 0.1),
                }
            )

        scorer = AdvancedKeywordScorer(target_geo="PE", target_intent="transactional")

        # Ejecutar scoring mÃºltiples veces para verificar consistencia
        all_results = []
        for run in range(3):
            results = scorer.calculate_advanced_score(test_keywords.copy())
            all_results.append(results)

        # Analizar consistencia
        consistency_analysis = {}

        for i, kw_data in enumerate(test_keywords):
            keyword = kw_data["keyword"]
            scores = [run[i]["advanced_score"] for run in all_results]
            intent_weights = [run[i]["intent_weight"] for run in all_results]
            geo_weights = [run[i]["geo_weight"] for run in all_results]

            score_variance = statistics.variance(scores) if len(scores) > 1 else 0
            intent_correct = all(
                (
                    w >= 0.9
                    if kw_data["expected_intent"] == "transactional"
                    else (0.6 <= w < 0.9 if kw_data["expected_intent"] == "commercial" else w < 0.6)
                )
                for w in intent_weights
            )
            geo_correct = all(w >= 0.9 if kw_data["expected_geo"] else w < 0.9 for w in geo_weights)

            consistency_analysis[keyword] = {
                "score_variance": score_variance,
                "intent_accuracy": intent_correct,
                "geo_accuracy": geo_correct,
                "avg_score": statistics.mean(scores),
            }

            print(
                f"   ğŸ“ˆ {keyword[:30]}... Varianza: {score_variance:.2f}, Intent: {'âœ…' if intent_correct else 'âŒ'}, Geo: {'âœ…' if geo_correct else 'âŒ'}"
            )

        overall_variance = statistics.mean(
            [v["score_variance"] for v in consistency_analysis.values()]
        )
        intent_accuracy = (
            sum(1 for v in consistency_analysis.values() if v["intent_accuracy"])
            / len(consistency_analysis)
            * 100
        )
        geo_accuracy = (
            sum(1 for v in consistency_analysis.values() if v["geo_accuracy"])
            / len(consistency_analysis)
            * 100
        )

        return {
            "overall_score_variance": overall_variance,
            "intent_classification_accuracy": intent_accuracy,
            "geo_detection_accuracy": geo_accuracy,
            "consistency_rating": (
                "Alta" if overall_variance < 2.0 else "Media" if overall_variance < 5.0 else "Baja"
            ),
        }

    async def _analyze_keyword_distribution(self) -> dict:
        """Analiza la distribuciÃ³n y calidad de las keywords generadas"""
        print("\n3ï¸âƒ£ Analizando DistribuciÃ³n de Keywords...")

        scraper = create_scraper(country="PE", max_concurrent=3)

        try:
            # Generar muestra grande para anÃ¡lisis
            seeds = ["marketing digital", "seo", "publicidad", "redes sociales", "email marketing"]
            all_keywords = []

            for seed in seeds[:3]:  # Limitar para el test
                results = await scraper.expand_keywords([seed], max_concurrent=2)
                if seed in results:
                    all_keywords.extend(results[seed])

            if not all_keywords:
                return {"error": "No se pudieron generar keywords para anÃ¡lisis"}

            # AnÃ¡lisis de distribuciÃ³n
            word_counts = [len(kw.split()) for kw in all_keywords]
            unique_ratio = len(set(all_keywords)) / len(all_keywords)

            # AnÃ¡lisis de patrones
            pattern_analysis = {
                "informational": len(
                    [
                        k
                        for k in all_keywords
                        if any(term in k.lower() for term in ["que es", "como", "tutorial", "guia"])
                    ]
                ),
                "commercial": len(
                    [
                        k
                        for k in all_keywords
                        if any(term in k.lower() for term in ["curso", "precio", "mejor", "gratis"])
                    ]
                ),
                "transactional": len(
                    [
                        k
                        for k in all_keywords
                        if any(
                            term in k.lower()
                            for term in ["agencia", "empresa", "servicio", "contratar"]
                        )
                    ]
                ),
                "geo_targeted": len(
                    [
                        k
                        for k in all_keywords
                        if any(
                            term in k.lower()
                            for term in ["lima", "peru", "perÃº", "madrid", "mexico"]
                        )
                    ]
                ),
            }

            distribution_health = {
                "total_keywords": len(all_keywords),
                "unique_keywords": len(set(all_keywords)),
                "uniqueness_ratio": unique_ratio,
                "avg_word_count": statistics.mean(word_counts),
                "word_count_distribution": dict(Counter(word_counts)),
                "intent_distribution": pattern_analysis,
                "quality_score": self._calculate_distribution_quality(
                    unique_ratio, word_counts, pattern_analysis
                ),
            }

            print(
                f"   ğŸ“Š Total: {len(all_keywords)}, Ãšnicos: {len(set(all_keywords))} ({unique_ratio:.1%})"
            )
            print(f"   ğŸ“ Palabras promedio: {statistics.mean(word_counts):.1f}")
            print(
                f"   ğŸ¯ DistribuciÃ³n: Trans={pattern_analysis['transactional']}, Com={pattern_analysis['commercial']}, Info={pattern_analysis['informational']}"
            )

            return distribution_health

        finally:
            await scraper.close()

    def _calculate_distribution_quality(
        self, uniqueness: float, word_counts: list[int], patterns: dict
    ) -> str:
        """Calcula calidad de la distribuciÃ³n"""
        total = sum(patterns.values())

        quality_score = 0

        # Penalizar baja uniqueness
        if uniqueness >= 0.8:
            quality_score += 30
        elif uniqueness >= 0.6:
            quality_score += 20
        else:
            quality_score += 10

        # Bonus por diversidad de word counts
        avg_words = statistics.mean(word_counts)
        if 2.5 <= avg_words <= 4.0:
            quality_score += 25
        elif 2.0 <= avg_words <= 5.0:
            quality_score += 15
        else:
            quality_score += 5

        # Bonus por distribuciÃ³n balanceada de intents
        if total > 0:
            transactional_pct = patterns["transactional"] / total
            commercial_pct = patterns["commercial"] / total

            if transactional_pct >= 0.1:  # Al menos 10% transaccional
                quality_score += 20
            if commercial_pct >= 0.2:  # Al menos 20% comercial
                quality_score += 15
            if patterns["geo_targeted"] / total >= 0.2:  # Al menos 20% geo
                quality_score += 10

        if quality_score >= 80:
            return "Excelente"
        elif quality_score >= 60:
            return "Buena"
        elif quality_score >= 40:
            return "Regular"
        else:
            return "Baja"

    async def _analyze_geo_accuracy(self) -> dict:
        """Analiza la precisiÃ³n del geo-targeting"""
        print("\n4ï¸âƒ£ Analizando PrecisiÃ³n Geo-targeting...")

        # Test con keywords geo-especÃ­ficas conocidas
        geo_test_cases = [
            {"keyword": "marketing digital lima", "country": "PE", "should_match": True},
            {"keyword": "agencia seo madrid", "country": "ES", "should_match": True},
            {"keyword": "curso marketing online", "country": "PE", "should_match": False},
            {"keyword": "publicidad digital mexico", "country": "MX", "should_match": True},
            {"keyword": "marketing digital", "country": "PE", "should_match": False},
        ]

        correct_predictions = 0
        total_tests = len(geo_test_cases)

        for test in geo_test_cases:
            scorer = AdvancedKeywordScorer(
                target_geo=test["country"], target_intent="transactional"
            )

            test_data = [
                {"keyword": test["keyword"], "trend_score": 70, "volume": 2000, "competition": 0.5}
            ]

            results = scorer.calculate_advanced_score(test_data)
            geo_weight = results[0]["geo_weight"]

            predicted_match = geo_weight >= 0.9
            is_correct = predicted_match == test["should_match"]

            if is_correct:
                correct_predictions += 1

            print(
                f"   ğŸŒ {test['keyword']} ({test['country']}): {geo_weight:.2f} - {'âœ…' if is_correct else 'âŒ'}"
            )

        accuracy = (correct_predictions / total_tests) * 100

        return {
            "test_cases": total_tests,
            "correct_predictions": correct_predictions,
            "accuracy_percentage": accuracy,
            "reliability": "Alta" if accuracy >= 90 else "Media" if accuracy >= 70 else "Baja",
        }

    async def _analyze_intent_accuracy(self) -> dict:
        """Analiza la precisiÃ³n de clasificaciÃ³n de intenciones"""
        print("\n5ï¸âƒ£ Analizando PrecisiÃ³n de Intenciones...")

        intent_test_cases = [
            {"keyword": "agencia marketing digital", "expected": "transactional"},
            {"keyword": "contratar consultor seo", "expected": "transactional"},
            {"keyword": "curso marketing digital precio", "expected": "commercial"},
            {"keyword": "mejor herramientas email marketing", "expected": "commercial"},
            {"keyword": "que es marketing digital", "expected": "informational"},
            {"keyword": "como hacer seo", "expected": "informational"},
        ]

        scorer = AdvancedKeywordScorer(target_geo="PE", target_intent="transactional")

        intent_accuracy = {"transactional": 0, "commercial": 0, "informational": 0}
        intent_totals = {"transactional": 0, "commercial": 0, "informational": 0}

        for test in intent_test_cases:
            test_data = [
                {"keyword": test["keyword"], "trend_score": 70, "volume": 2000, "competition": 0.5}
            ]

            results = scorer.calculate_advanced_score(test_data)
            intent_weight = results[0]["intent_weight"]

            # Clasificar basado en weight
            if intent_weight >= 0.9:
                predicted = "transactional"
            elif intent_weight >= 0.6:
                predicted = "commercial"
            else:
                predicted = "informational"

            expected = test["expected"]
            is_correct = predicted == expected

            intent_totals[expected] += 1
            if is_correct:
                intent_accuracy[expected] += 1

            print(
                f"   ğŸ¯ {test['keyword']}: {intent_weight:.2f} ({predicted}) - {'âœ…' if is_correct else 'âŒ'}"
            )

        # Calcular accuracy por tipo
        accuracy_by_type = {}
        overall_correct = 0
        overall_total = 0

        for intent_type in intent_accuracy:
            if intent_totals[intent_type] > 0:
                accuracy = (intent_accuracy[intent_type] / intent_totals[intent_type]) * 100
                accuracy_by_type[intent_type] = accuracy
                overall_correct += intent_accuracy[intent_type]
            else:
                accuracy_by_type[intent_type] = 0
            overall_total += intent_totals[intent_type]

        overall_accuracy = (overall_correct / overall_total) * 100 if overall_total > 0 else 0

        return {
            "accuracy_by_intent": accuracy_by_type,
            "overall_accuracy": overall_accuracy,
            "test_cases": overall_total,
            "reliability": (
                "Alta" if overall_accuracy >= 85 else "Media" if overall_accuracy >= 70 else "Baja"
            ),
        }

    async def _cross_validate_results(self) -> dict:
        """ValidaciÃ³n cruzada con diferentes configuraciones"""
        print("\n6ï¸âƒ£ Ejecutando ValidaciÃ³n Cruzada...")

        test_keywords = [
            {
                "keyword": "marketing digital lima",
                "trend_score": 75,
                "volume": 2000,
                "competition": 0.4,
            },
            {"keyword": "curso seo online", "trend_score": 60, "volume": 1500, "competition": 0.6},
            {
                "keyword": "agencia publicidad madrid",
                "trend_score": 80,
                "volume": 3000,
                "competition": 0.3,
            },
        ]

        # Test con diferentes configuraciones
        configs = [
            {"geo": "PE", "intent": "transactional"},
            {"geo": "ES", "intent": "transactional"},
            {"geo": "PE", "intent": "commercial"},
        ]

        cross_validation_results = {}

        for config in configs:
            scorer = AdvancedKeywordScorer(target_geo=config["geo"], target_intent=config["intent"])
            results = scorer.calculate_advanced_score(test_keywords.copy())

            config_key = f"{config['geo']}_{config['intent']}"
            cross_validation_results[config_key] = {
                "scores": [r["advanced_score"] for r in results],
                "avg_score": statistics.mean([r["advanced_score"] for r in results]),
                "geo_matches": sum(1 for r in results if r["geo_weight"] >= 0.9),
                "intent_alignment": sum(
                    1
                    for r in results
                    if (
                        r["intent_weight"] >= 0.9
                        if config["intent"] == "transactional"
                        else r["intent_weight"] >= 0.6
                    )
                ),
            }

            print(
                f"   âš™ï¸ Config {config_key}: Avg Score {cross_validation_results[config_key]['avg_score']:.1f}"
            )

        return cross_validation_results

    def _calculate_overall_confidence(self) -> str:
        """Calcula confianza general del sistema"""
        # Esta serÃ­a calculada basada en todos los anÃ¡lisis
        # Por ahora retornamos una estimaciÃ³n
        return "Alta (85-90%)"

    def _identify_limitations(self) -> list[str]:
        """Identifica limitaciones del sistema"""
        return [
            "ğŸš« Dependencia de APIs externas (Google) - puede fallar si hay rate limiting",
            "ğŸ“Š Datos de volumen estimados - no son reales de Google Ads",
            "ğŸŒ Geo-targeting bÃ¡sico - no considera geo-targeting semÃ¡ntico avanzado",
            "â±ï¸ Sin datos temporales - no considera estacionalidad real",
            "ğŸ¤– ClasificaciÃ³n de intent heurÃ­stica - no usa ML/NLP avanzado",
            "ğŸ“ˆ Trends data limitado - dependiente de disponibilidad de Google Trends",
            "ğŸ”„ Sin validaciÃ³n histÃ³rica - falta data para probar precisiÃ³n a largo plazo",
        ]

    def _generate_recommendations(self) -> list[str]:
        """Genera recomendaciones para mejorar confiabilidad"""
        return [
            "âœ… Implementar cache robusto para reducir dependencia de APIs",
            "ğŸ“Š Integrar con Google Ads API para datos reales de volumen/competencia",
            "ğŸ¤– Desarrollar modelo ML para clasificaciÃ³n de intents mÃ¡s precisa",
            "ğŸ“ˆ Agregar anÃ¡lisis de estacionalidad basado en datos histÃ³ricos",
            "ğŸ” Implementar validaciÃ³n continua con mÃ©tricas de negocio",
            "ğŸŒ Mejorar geo-targeting con anÃ¡lisis semÃ¡ntico avanzado",
            "ğŸ“‹ Crear dashboard de monitoreo de confiabilidad en tiempo real",
            "ğŸ§ª Implementar A/B testing para validar mejoras del algoritmo",
        ]

    def _print_reliability_report(self, report: dict):
        """Imprime reporte completo de confiabilidad"""
        print("\n" + "=" * 60)
        print("ğŸ“‹ REPORTE DE CONFIABILIDAD - RESUMEN EJECUTIVO")
        print("=" * 60)

        print(f"\nğŸ¯ CONFIANZA GENERAL: {report['overall_confidence']}")

        print("\nğŸ“Š MÃ‰TRICAS CLAVE:")
        print(f"   â€¢ Consistencia Scoring: {report['scoring_consistency']['consistency_rating']}")
        print(f"   â€¢ PrecisiÃ³n Geo-targeting: {report['geo_accuracy']['reliability']}")
        print(f"   â€¢ PrecisiÃ³n Intenciones: {report['intent_accuracy']['reliability']}")
        print(
            f"   â€¢ Calidad DistribuciÃ³n: {report['distribution_health'].get('quality_score', 'N/A')}"
        )

        print("\nâš ï¸  LIMITACIONES PRINCIPALES:")
        for limitation in report["limitations"][:4]:
            print(f"   {limitation}")

        print("\nğŸ’¡ RECOMENDACIONES TOP:")
        for rec in report["recommendations"][:4]:
            print(f"   {rec}")

        print("\nğŸ” CONCLUSIÃ“N:")
        print("   El sistema muestra alta confiabilidad para uso empresarial con las")
        print("   limitaciones identificadas. Recomendamos implementar las mejoras")
        print("   sugeridas para maximizar precisiÃ³n y robustez.")


async def main():
    """Ejecuta anÃ¡lisis completo de confiabilidad"""
    analyzer = KeywordReliabilityAnalyzer()
    await analyzer.analyze_full_reliability()


if __name__ == "__main__":
    asyncio.run(main())
